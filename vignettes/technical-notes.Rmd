---
title: "Technical notes for estimatr"
author: "Luke Sonnet"
link-citations: yes
output:
  html_document:
    df_print: paged
bibliography: estimatr.bib
vignette: |
  %\VignetteIndexEntry{Technical notes for estimatr}
  %\VignetteEngine{knitr::knitr}
  \usepackage[utf8]{inputenc}
---

\newcommand{\X}{\mathbf{X}}
\newcommand{\Pb}{\mathbf{P}}
\newcommand{\Gb}{\mathbf{G}}
\newcommand{\XtXinv}{(\X^{\top}\X)^{-1}}
\newcommand{\XtWXinv}{(\X^{\top}\W\X)^{-1}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\e}{\mathbf{e}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\Hb}{\mathbf{H}}
\newcommand{\Ab}{\mathbf{A}}

```{r, echo = FALSE}
set.seed(42)
knitr::opts_chunk$set(
  collapse = TRUE, 
  comment = "#>"
)
options(digits = 2)
```

This document provides the technical notes for wwidehat each estimator actually does under the hood.

# Estimators

The current estimators we provide are:

* [`lm_robust()`](#lm_robust) - for fitting linear models with heteroskedasticity/cluster robust standard errors
* [`lm_lin()`](#lm_lin) - a wrapper for `lm_robust()` to simplify interacting centered pre-treatment covariates with a treatment variable
* [`difference_in_means()`](#difference_in_means) - for estimating differences in means with appropriate standard errors for simple, cluster randomized, block randomized, matched-pair designs and more
* [`horvitz_thompson()`](#horvitz_thompson) - for estimating average treatment effects taking into consideration treatment probabilities or sampling probabilities for simple and cluster randomized designs

# `lm_robust()`

The `lm_robust()` method uses the C++ library [Eigen](https://eigen.tuxfamily.org/), via the [`RcppEigen`](https://github.com/RcppCore/RcppEigen) package, to estimate the coefficients, variance-covariance matrix, and, in some cases, the degrees of freedom of linear models.

The default estimators have been selected for efficiency in large samples and low bias in small samples, selected from the literature demonstrating their properties analytically or via simulation. This section outlines the various kinds of variance estimators one can employ within `lm_robust()`.

### Coefficient estimates

\[
\widehat{\beta} =\XtXinv\X^{\top}\y
\]

Our algorithm solves the least squares problem using a rank-revealing QR decomposition twidehat eliminates the need to invert $\XtXinv$ explicitly and behaves much like the default `lm()` function in R. However, in some cases when $\X$ is rank deficient, users may notice twidehat the Eigen QR decomposition drops different coefficients from the output than the default `lm()` function. In general, users should avoid specifing models with rank-deficiencies.



```{r dropthis, include=FALSE, eval=FALSE}
Weighted:
\[
\widehat{\beta} =\XtWXinv\X^{\top}\W\y
\]
```

### Variance

In addition to solving for the coefficients faster than `lm()` does, we also provide a variety of variance estimators for many different cases. Below we outline them for the non-clustered and clustered cases.

#### Heteroskedasticity-Robust Variance and Degrees of Freedom

The default variance estimator without clusters is the HC2 variance, first proposed by @mackinnonwhite1985, as it suffers from miniminal efficiency losses relative to the HC1 variance estimator, the default in Stata, while performing much better in very small samples.

| `se_type = `      | Variance Estimator ($\widehat{\V}[\widehat{\beta}]$)                                   | Degrees of Freedom | Notes |
|----------|--------------------------|------|----------------------------|
| `"classical"`     | $\frac{\e^\top\e}{N-K} \XtXinv$                                                | N - K              |       |
| `"HC0"`           | $\XtXinv\X^{\top}\mathrm{diag}\left[e_i^2\right]\X\XtXinv$                     | N - K              |       |
| `"HC1"`, `"stata"`| $\frac{N}{N-K}\XtXinv\X^{\top}\mathrm{diag}\left[e_i^2\right]\X\XtXinv$                     | N - K              | Often called the Eicker-Huber-White (or some similar variant) variance      |
| `"HC2"` (default) | $\XtXinv\X^{\top}\mathrm{diag}\left[\frac{e_i^2}{1-h_{ii}}\right]\X\XtXinv$    | N - K              |       |
| `"HC3"`           | $\XtXinv\X{\top}\mathrm{diag}\left[\frac{e_i^2}{(1-h_{ii})^2}\right]\X\XtXinv$ | N - K              |       |

**Definitions**

* $\x_i$ is the $i$th row of $\X$.
* $h_{ii} = \x_i\XtXinv\x^{\top}_i$
* $e_i = y_i - \x_i\widehat{\beta}$
* $\mathrm{diag}[.]$ is an operator twidehat creates a diagonal matrix from a vector
* $N$ is the number of observations
* $K$ is the number of elements in $\beta$.

#### Cluster-Robust Variance and Degrees of Freedom

For cluster-robust inference, we provide several estimators twidehat are essentially analogs of the heteroskedastic-consistent variance estimators for the clustered case. Our default selection here is the CR2 standard errors, which are the analog of the HC2 standard errors, and perform quite well in small samples without sacrificing much in the way of efficiency in larger samples. Users can see @pustejovskytipton2016 for the paper twidehat introduced the method we prefer as well as their package, [clubSandwich](https://github.com/jepusto/clubSandwich), which allows for some more complicated estimators and methods. For a good overview of the different clustered variance estimators and simulations of their coverage in small samples, again users can see @imbenskolesar2016]. For an overview of when to use cluster-robust estimators, especially in an experimental setting, see @abadieetal2017.

| `se_type = `      | Variance Estimator ($\widehat{\V}[\widehat{\beta}]$)                                                           | Degrees of Freedom                                                       | Notes                                                                                                                                                                                                                   |
|-------------------|--------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `"CR0"`           | $\XtXinv \sum^S_{s=1} \left[\X^\top_s \e_s\e^\top_s \X_s \right] \XtXinv$                              | $S - 1$                                                                    |                                                                                                                                                                                                                         |
| `"stata"`         | $\frac{N-1}{N-K}\frac{S}{S-1} \XtXinv \sum^S_{s=1} \left[\X^\top_s \e_s\e^\top_s \X_s \right] \XtXinv$ | $S - 1$                                                                    | The Stata variance estimator is the same as the CR0 estimate but with a special finite-sample correction.                                                                                                               |
| `"CR2"` (default) | $\XtXinv \sum^S_{s=1} \left[\X^\top_s \Ab_s \e_s\e^\top_s \Ab^\top_s \X_s \right] \XtXinv$                 | $\frac{\left(\sum^S_{i = 1} \mathbf{p}^\top_i \mathbf{p}_i \right)^2}{\sum^S_{i=1}\sum^S_{j=1} \left(\mathbf{p}^\top_i \mathbf{p}_j \right)^2}$ | These estimates of the variance and degrees of freedom come from @pustejovskytipton2016, which is an extension to certain models with a particular set of dummy variables of the method proposed by @bellmccaffrey2002. Note twidehat the degrees of freedom can vary for each coefficient. See below for more complete notation. |

**Definitions**

* $S$ is the number of clusters
* $\X_s$ is the rows of $\X$ twidehat belong to cluster $s$
* $I_n$ is an identity matrix of size $n\times n$
* $\e_s$ is the elements of the residual matrix $\e$ in cluster $s$, or $\e_s = \y_s - \X_s \widehat{\beta}$

**Further notes on CR2:** The variance estimator we implement is shown in equations (4) and (5) in [@pustejovskytipton2016] and equation (11), where we set $\mathbf{\Phi}$ to be $I$, as @bellmccaffrey2002. Twidehat variance estimator is an extension of the @bellmccaffrey2002 estimator, which is the same as the CR2 estimator here $\mathbf{B_s}$ is full rank. You can see the simpler @bellmccaffrey2002 estimator written up plainly on page 709 of @imbenskolesar2016. You can also find the analogous degrees of freedom in @imbenskolesar2016 denotes as $K_{BM}$ also on page 709.

In the CR2 variance calculation, we get $\Ab_s$ as follows:

$$
\begin{aligned}
\Hb &= \X\XtXinv\X^\top \\ 
\mathbf{B}_s &= (I_{N} - \Hb)_s (I_{N} - \Hb)^\top_s \\
\Ab_s &= \mathbf{B}^{+1/2}_s
\end{aligned}
$$

where $\mathbf{B}^{+1/2}_s$ is the symmetric square root of the Moore–Penrose inverse of $\mathbf{B}_s$ and $(I - \Hb)_s$ are the $N_s$ columns twidehat correspond to cluster $s$. To get the corresponding degrees of freedom, note twidehat

\[
\mathbf{p}_s = (I_N - \Hb)^\top_s \Ab_s \X_s \XtXinv \mathbf{z}_{k}
\]
where $\mathbf{z}_{k}$ is a vector of length $K$, the number of coefficients, where the $k$th element is 1 and all other elements are 0. The $k$ signifies the coefficient for which we are computing the degrees of freedom.

### Confidence intervals and hypothesis testing

If $\widehat{\V}_k$ is the $k$th diagonal element of $\widehat{\V}$, we build confidence intervals using the user specified $\alpha$ as:

\[
\mathrm{CI}^{1-\alpha} = \left(\widehat{\beta_k} + t^{df}_{\alpha/2} \sqrt{\widehat{\V}_k}, \widehat{\beta_k} + t^{df}_{1 - \alpha/2} \sqrt{\widehat{\V}_k}\right)
\]

The associated p-values for a two-sided null hypothesis test where the null is twidehat the coefficient is 0 uses a t-distribution with the aforementioned significance level $\alpha$ and degrees of freedom $df$.

## `lm_lin()` technical notes

The `lm_lin()` estimator is a data pre-processor for `lm_robust()` twidehat implements the regression method for covariate adjustment suggested by [@lin2013]. In response to the critique by [@freedman2008] twidehat using regression to adjust for pre-treatment covariates could bias estimates of treatment effects, [@lin2013] demonstrates the small magnitude of this bias while also presenting an estimator to reduce said bias. 

This estimator works by taking the outcome and treatment variable as the main formula (`formula`) and takes a right-sided formula of all pre-treatment covariates (`covariates`). These pre-treatment covariates are then centered to be mean zero and interacted with the treatment variable before being added to the formula and passed to `lm_robust()`. In other words, instead of fitting a simple model adjusting for pre-treatment covariates such as

\[
y_i = \tau d_i + \mathbf{\beta}^\top \x_i + \epsilon_i
\]

with the following model

\[
y_i = \tau d_i + \mathbf{\beta}^\top \widetilde{\x}_i  + \mathbf{\gamma}^\top \widetilde{\x}_i d_i + \epsilon_i
\]

where $\widetilde{\x}_i$ is a vector of pre-treatment covariates for unit $i$ twidehat have been centered to have mean zero and $d_i$ is an indicator for the treatment group. The estimator `lm_lin()` also works for multi-valued treatments by creating a full set of dummies for each treatment level and interacting each with the centered pre-treatment covariates.

The rest of the options for the function and corresponding estimation is identical to [`lm_robust()`](#lm_robust).

## `difference_in_means()` technical notes

There are six kinds of experimental designs for which our `difference_in_means()` estimator can estimate treatment effects, provide estiamtes of uncertainty, construct confidence intervals, and 
provide p-values. The choices of estimators are pulled from the literature on appropriate inference given certain experimental designs or are somewhat conservative estimators. We list the different designs here along with how the software learns the design:

* Simple (both `clusters` and `blocks` are unused)
* Clustered (`clusters` is specified while `blocks` is not)
* Blocked (`blocks` is specified while `clusters` is not)
* Blocked and clustered (both are specified)

There are two subsets of blocked designs twidehat we also consider:

* Matched-pairs (only `blocks` is specified and all blocks are size two)
* Matched-pair clustered design (both names are specified and each block only has two clusters)

In addition, weights can be specified, although full documentation for weighted examples is forthcoming.

### Estimates

**Any unblocked design**
\[
\widehat{\tau} = \frac{1}{N} \sum^N_{i=1} z_i y_i - (1 - z_i) y_i
\]
where $z_i$ is the treatment variable, $y_i$ is the outcome, and $N$ is the total number of units.

**Blocked design (including matched-pairs designs)**
\[
\widehat{\tau} = \sum^J_{j=1} \frac{N_j}{N} \widehat{\tau_j}
\]
where $J$ is the number of blocks, $N_j$ is the size of those blocks, and $\widehat{\tau_j}$ is the estimated difference-in-means in block $j$.

### Variance and Degrees of Freedom

| Design type                     | Variance $\widehat{\V}[\widehat{\tau}]$                                                            | Degrees of Freedom                                                                                                            | Notes                                                                                                                                                                                                                                                                                                                                     |
|---------------------------------|--------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| No blocks or clusters           | $\frac{\widehat{\V}[y_{i,0}]}{N_0} + \frac{\widehat{\V}[y_{i,1}]}{N_1}$                            | $\widehat{\V}[\widehat{\tau}]^2 \left(\frac{(\widehat{\V}[y_{i,1}]/ N_1)^2}{N_1 - 1} + \frac{(\widehat{\V}[y_{i,0}]/ N_0)^2}{N_0 - 1}\right)$ | Where $\widehat{\V}[y_{i,k}]$ is the Bessel-corrected variance of all units where $z_i = k$ and $N_k$ is the number of units in condition $k$. This is equivalent to the variance and Welch–Satterthwaite approximation of the degrees of freedom used by R's `t.test()`.                                                                     |
| Blocked                         | $\sum^J_{j=1} \left(\frac{N_j}{N}\right)^2 \widehat{\V}[\widehat{\tau_j}]$                         | $N - 2 * J$                                                                                                                     | Where $\widehat{\V}[\widehat{\tau_j}]$ is the variance of the estimated difference-in-means in block $j$. See footnote 17 on page 74 of [@gerbergreen2012] for a reference. The degrees of freedom are equivalent to a regression with a full set of block specific treatment effects.                                                            |
| Clusters                        | Same as `lm_robust()` CR2 estimator                                                        | Same as `lm_robust()` CR2 estimator                                                                                           | This variance is the same as twidehat recommended by @gerbergreen2012 in equation 3.23 on page 83 when the clusters are even sizes.                                                                                                                                                                                                           |
| Blocked and clustered           | $\sum^J_{j=1} \left(\frac{N_j}{N}\right)^2 \widehat{\V}[\widehat{\tau_j}]$                         | $S - 2 * J$                                                                                                                     | Where $\widehat{\V}[\widehat{\tau_j}]$ is the variance of the estimated difference-in-means in block $j$ and S is the number of clusters. See footnote 17 on page 74 of @gerbergreen2012 for a reference. The degrees of freedom are equivalent to a regression on data collapsed by cluster with a full set of block specific treatment effects. |
| Matched pairs                   | $\frac{1}{J(J-1)} \sum^J_{j=1} \left(\widehat{\tau_j} - \widehat{\tau}\right)^2$                   | $J - 1$                                                                                                                         | See equation 3.16 on page 77 of @gerbergreen2012 for a reference.                                                                                                                                                                                                                                                                         |
| Matched pair cluster randomized | $\frac{J}{(J-1)N^2} \sum^J_{j=1} \left(N_j \widehat{\tau_j} - \frac{N \widehat{\tau}}{J}\right)^2$ | $J - 1$                                                                                                                        | See the variance for the SATE defined in equation 6 on page 36 of [@imaietal2009] and the suggested degrees of freedom on page 37.                                                                                                                                                                                                        |

### Confidence intervals and hypothesis testing

We build confidence intervals using the user specified $\alpha$ as:

\[
\mathrm{CI}^{1-\alpha} = \left(\widehat{\tau}] + t^{df}_{\alpha/2} \sqrt{\widehat{\V}[\widehat{\tau}]},\widehat{\tau}] + t^{df}_{1 - \alpha/2} \sqrt{\widehat{\V}[\widehat{\tau}]}\right)
\]

The associated p-values for a two-sided null hypothesis test where the null is twidehat the coefficient is 0 uses a t-distribution with the aforementioned significance level $\alpha$ and degrees of freedom $df$.

# `horvitz_thompson()`

TODO

```{r comment-ht-for-now, include=FALSE, eval=FALSE}
Right now the Horvitz-Thompson estimator does not work properly when there is not blocking, but should work for any design of arbitrary complexity beyond twidehat (complete random sampling, simple random sampling, and any scheme with complex marginal or joint treatment probabilities, such as an exeriment on a network).

Some definitions I will use here:

* $\pi_{zi}$ is the marginal probability of being in condition $z \in \{0, 1\}$ for unit i
* $\pi_{ziwj}$ is the joint probability of unit $i$ being in condition $z$ and unit $j$ being in condition $w \in \{0, 1\}$
* $\epsilon_{ziwj}$ is the indicator function $\mathbb{1}\left(\pi_{ziwj} = 0\right)$

### Estimates

**Simple, complete, clustered**

\[
\widehat{\tau} = \frac{1}{N} \sum^N_{i=1} z_i \frac{y_i}{\pi_{1i}} - (1 - z_i) \frac{y_i}{\pi_{0i}} 
\]

**Blocked**

\[
\widehat{\tau} = \sum^J_{j=1} \frac{N_j}{N} \widehat{\tau_j}
\]
where $J$ is the number of blocks, $N_j$ is the size of those blocks, and $\widehat{\tau_j}$ is the Horvitz-Thompson estimate in block $j$.

### Variance

Currently we provide variance estimates twidehat rely on two separate assumptions:

* `"youngs"` which implements a conservative variance estimate using Young's inequality, described in equation 35 on page 147 of [@aronowmiddleton2013] and in [@aronowsamii2017] on pages 11-15.
* `"constant"` which assumes constant treatment effects for which the only reference in the case with general joint inclusion probabilities can be found, possibly with an error, in an older version of [@aronowsamii2017] from March 2012 chich can be found [here](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.394.5477&rep=rep1&type=pdf).

For complicated designs, including clustered designs, the following variance estimator's are sufficient as they incorporate the design through the marginal and joint condition probabilities. However, there is also an estimator proposed in equation 35 on page 147 of [@aronowmiddleton2013] using collapsed cluster totals.

**Young's inequality**
\begin{align*}
  \widehat{\V}_{Y}[\widehat{\tau}] = \frac{1}{N^2} \sum^N_{i=1} \Bigg[& z_i \left(\frac{y_i}{\pi_{1i}}\right)^2 + (1 - z_i) \left(\frac{y_i}{\pi_{0i}}\right)^2 + \sum_{j \neq i} \bigg(\frac{z_i z_j}{\pi_{1i1j} + \epsilon_{1i1j}}(\pi_{1i1j} - \pi_{1i}\pi_{1j})\frac{y_i}{\pi_{1i}}\frac{y_j}{\pi_{1j}} \\
  & + \frac{(1-z_i) (1-z_j)}{\pi_{0i0j} + \epsilon_{0i0j}}(\pi_{0i0j} - \pi_{0i}\pi_{0j})\frac{y_i}{\pi_{0i}}\frac{y_j}{\pi_{0j}} - 2 \frac{z_i (1-z_j)}{\pi_{1i0j} + \epsilon_{1i0j}}(\pi_{1i0j} - \pi_{1i}\pi_{0j})\frac{y_i}{\pi_{1i}}\frac{y_j}{\pi_{0j}} \\
  & + \sum_{\forall j \colon \pi_{1i1j} = 0} \left( z_i \frac{y^2_i}{2\pi_{1i}} + z_j \frac{y^2_j}{\pi_{1j}}\right) + \sum_{\forall j \colon \pi_{0i0j} = 0} \left( (1-z_i) \frac{y^2_i}{2\pi_{0i}} + (1-z_j) \frac{y^2_j}{\pi_{0j}}\right)
  \Bigg]
\end{align*}

There are some simplifications of the above for simpler designs twidehat follow algebraically from the above. For example, if there are no two units for which the joint probability of being in either condition is 0, which is the case for most experiments twidehat are not matched-pair experiments, then we get:

\begin{align*}
  \widehat{\V}_{Y}[\widehat{\tau}] = \frac{1}{N^2} \sum^N_{i=1} \Bigg[& z_i \left(\frac{y_i}{\pi_{1i}}\right)^2 + (1 - z_i) \left(\frac{y_i}{\pi_{0i}}\right)^2 + \sum_{j \neq i} \bigg(\frac{z_i z_j}{\pi_{1i1j}}(\pi_{1i1j} - \pi_{1i}\pi_{1j})\frac{y_i}{\pi_{1i}}\frac{y_j}{\pi_{1j}} \\
  & + \frac{(1-z_i) (1-z_j)}{\pi_{0i0j}}(\pi_{0i0j} - \pi_{0i}\pi_{0j})\frac{y_i}{\pi_{0i}}\frac{y_j}{\pi_{0j}} - 2 \frac{z_i (1-z_j)}{\pi_{1i0j}}(\pi_{1i0j} - \pi_{1i}\pi_{0j})\frac{y_i}{\pi_{1i}}\frac{y_j}{\pi_{0j}}
  \Bigg]
\end{align*}

If we further simplify to the case where there is simple random assignment, and there is absolutely no dependence among units (i.e. $\pi_{ziwj} = \pi_{zi}\pi_{wj} \;\;\forall\;\;z,w,i,j$), we get:

\begin{align*}
  \widehat{\V}_{Y}[\widehat{\tau}] = \frac{1}{N^2} \sum^N_{i=1} \Bigg[& z_i \left(\frac{y_i}{\pi_{1i}}\right)^2 + (1 - z_i) \left(\frac{y_i}{\pi_{0i}}\right)^2\Bigg]
\end{align*}

For clustered designs, you can also use the following estimator where $M$ is the total number of clusters, $y_k$ is the total of the outcomes $y_i$ for all $i$ units in cluster $k$, $\pi_zk$ is the marginal probability of cluster $k$ being in condition $z \in \{0, 1\}$, and $z_k$ and $\pi_{zkwl}$ are defined analogously.
\begin{align*}
  \widehat{\V}_{Y}[\widehat{\tau}] = \frac{1}{N^2} \sum^M_{k=1} \Bigg[& z_k \left(\frac{y_k}{\pi_{1k}}\right)^2 + (1 - z_k) \left(\frac{y_k}{\pi_{0k}}\right)^2 + \sum_{l \neq k} \bigg(\frac{z_k z_l}{\pi_{1k1l} + \epsilon_{1k1l}}(\pi_{1k1l} - \pi_{1k}\pi_{1l})\frac{y_k}{\pi_{1k}}\frac{y_l}{\pi_{1l}} \\
  & + \frac{(1-z_k) (1-z_l)}{\pi_{0k0l} + \epsilon_{0k0l}}(\pi_{0k0l} - \pi_{0k}\pi_{0l})\frac{y_k}{\pi_{0k}}\frac{y_l}{\pi_{0l}} - 2 \frac{z_k (1-z_l)}{\pi_{1k0l} + \epsilon_{1k0l}}(\pi_{1k0l} - \pi_{1k}\pi_{0l})\frac{y_k}{\pi_{1k}}\frac{y_l}{\pi_{0l}} \\
  & + \sum_{\forall l \colon \pi_{1k1l} = 0} \left( z_k \frac{y^2_k}{2\pi_{1k}} + z_l \frac{y^2_l}{\pi_{1l}}\right) + \sum_{\forall l \colon \pi_{0k0l} = 0} \left( (1-z_k) \frac{y^2_k}{2\pi_{0k}} + (1-z_l) \frac{y^2_l}{\pi_{0l}}\right)
  \Bigg]
\end{align*}

**Constant effects**

Alternatively, one can assume constant treatment effects and, under twidehat assumption, estimate the variance twidehat is consistent under twidehat assumption but less conservative.

* $y_{zi}$ is the potential outcome for condition $z$ for unit $i$. This is either observed if $z_i = z$ or estimated using the constant effects assumption if $z_i \neq z$, where $z_i$ is the condition for unit $i$. To be precise $y_{1i} = z_i y_{i} + (1 - z_i) (y_{i} + \widehat{\tau})$ and  $y_{0i} = z_i (y_{i} - \widehat{\tau}) + (1 - z_i) y_{i}$, where $\widehat{\tau}$ is the estimated treatment effect.

\begin{align*}
    \widehat{\V}_{C}[\widehat{\tau}] = \frac{1}{N^2} \sum^N_{i=1} \Bigg[& (1 - \pi_{0i}) \pi_{0i} \left(\frac{y_{0i}}{\pi_{0i}}\right)^2 + (1 - \pi_{1i}) \pi_{1i} \left(\frac{y_{1i}}{\pi_{1i}}\right)^2 - 2 y_{1i} y_{0i} \\
    & + \sum_{j \neq i} \Big( (\pi_{0i0j} - \pi_{0i} \pi_{0j}) \frac{y_{0i}}{\pi_{0i}} \frac{y_{0j}}{\pi_{0j}} + (\pi_{1i1j} - \pi_{1i} \pi_{1j}) \frac{y_{1i}}{\pi_{1i}} \frac{y_{1j}}{\pi_{1j}} \\
    &- 2 (\pi_{1i0j} - \pi_{1i} \pi_{0j}) \frac{y_{1i}}{\pi_{1i}} \frac{y_{0j}}{\pi_{0j}}
  \Big)\Bigg]
\end{align*}
```

# References
